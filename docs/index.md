# Evaluation
We evaluated our system against different LLMs with direct prompts in an urban search and rescue scenario.
![image of the scenario](/evaluation/scenario.png)

We asked [50 questions](/evaluation/questions.txt). The direct prompts and results can be found [here](/evaluation).

![bar chart of results](/evaluation/plot.png)
This overview shows how many questions each system answered, optimal, correct or incorrect. Further details on which questions were answered correct can be found [here](/evaluation)



# This page is currently in development and features the results of the evaluation. Later updates will have releases of source code in the future.

